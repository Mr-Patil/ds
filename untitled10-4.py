# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vu9bQ2ZM8oTGtYvsTutXFvtRR-rOQTjJ
"""

import pandas as pd
import numpy as np
from pandas_datareader import data as pdr
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

import yfinance as yf
import pandas as pd

# define start and end dates
start_date = '2022-01-01'
end_date = '2022-12-31'

# define list of top 15 companies
top_15_companies = ['AAPL', 'MSFT', 'GOOGL', 'GOOG', 'TSM', 'NVDA', 'JPM', 'JNJ', 'V', 'UNH', 'PG', 'MA', 'HD', 'BAC']
#top_15_companies = {'AAPL': 'Apple Inc.', 'MSFT': 'Microsoft Corporation', 'AMZN': 'Amazon.com, Inc.', 'GOOG': 'Alphabet Inc.', 'FB': 'Meta Platforms, Inc.', 'TSLA': 'Tesla, Inc.', 'JPM': 'JPMorgan Chase & Co.', 'JNJ': 'Johnson & Johnson', 'NVDA': 'NVIDIA Corporation', 'V': 'Visa Inc.', 'UNH': 'UnitedHealth Group Incorporated', 'MA': 'Mastercard Incorporated', 'HD': 'The Home Depot, Inc.', 'PYPL': 'PayPal Holdings, Inc.', 'BAC': 'Bank of America Corporation'}

# create an empty list to store dataframes
df_list = []

# loop through each symbol to retrieve data and store in list
for symbol in top_15_companies:
    data = yf.download(symbol, start=start_date, end=end_date)
    data['Symbol'] = symbol
    df_list.append(data)

# concatenate dataframes into a single dataframe
df = pd.concat(df_list)

# reset index
df.reset_index(inplace=True)

# display dataframe
print(df.head())

# concatenate dataframes and reset index
df = pd.concat(df_list).reset_index()

# rename columns
df = df.rename(columns={'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Adj Close': 'adj_close', 'Volume': 'volume', 'Symbol': 'symbol'})

# convert date to datetime object and set as index
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date')

# concatenate all dataframes into a single dataframe
df_combined = pd.concat(df_list, axis=0)

# reset the index
df_combined = df_combined.reset_index()

# keep only necessary columns
df_combined = df_combined[['Symbol', 'Date', 'Close']]

# rename the columns
df_combined.columns = ['symbol', 'date', 'close']

# convert date column to datetime object
df_combined['date'] = pd.to_datetime(df_combined['date'])

# sort the dataframe by symbol and date
df_combined = df_combined.sort_values(['symbol', 'date'])

# EDA
# Check for missing values
print(data.isna().sum())
# Check for outliers
sns.boxplot(data=data)
# Check for correlations
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
# Feature engineering
data['returns'] = data['Adj Close'].pct_change()
data['returns'].fillna(0, inplace=True)

# Split the data into training and testing sets
X = data.drop(['returns'], axis=1)
y = np.where(data['returns'] > 0, 1, -1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Convert categorical data to numerical using one-hot encoding
X = pd.get_dummies(X, columns=['Symbol'], prefix='symbol')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build Decision Tree Classifier
dtc = DecisionTreeClassifier(max_depth=5)
dtc.fit(X_train, y_train)
y_pred = dtc.predict(X_test)
print('Decision Tree Classifier Accuracy:', accuracy_score(y_test, y_pred))

from sklearn.metrics import classification_report

# Print a report of the classifier's performance
print(classification_report(y_test, y_pred))

# Build Random Forest Classifier
rfc = RandomForestClassifier()
params = {
    'n_estimators': [50, 100, 150],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 4, 6],
    'min_samples_leaf': [1, 2, 3]
}
grid_search = GridSearchCV(rfc, params, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
rfc_best = grid_search.best_estimator_
print('Random Forest Classifier Accuracy:', accuracy_score(y_test, rfc_best.predict(X_test)))

from sklearn.svm import SVC

# Build SVM Classifier
svm = SVC()
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
print('SVM Classifier Accuracy:', accuracy_score(y_test, y_pred))

# Build Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
print('Logistic Regression Classifier Accuracy:', accuracy_score(y_test, y_pred))

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

# Evaluate models using cross-validation
models = [('Decision Tree', dtc), ('Random Forest', rfc_best), ('SVM', svm), ('Logistic Regression', lr)]
results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))

# Compare models with boxplot
plt.boxplot(results, labels=names)
plt.title('Algorithm Comparison')
plt.show()

# Evaluate models using cross-validation
models = [('Decision Tree', dtc), ('Random Forest', rfc_best), ('SVM', svm), ('Logistic Regression', lr)]
results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')

# Plot the results
plt.figure(figsize=(10,6))
plt.title('Model Comparison')
plt.ylabel('Accuracy')
plt.xlabel('Model')
sns.barplot(x=names, y=[result.mean() for result in results], 
            yerr=[1.96 * result.std()/np.sqrt(len(result)) for result in results])
plt.show()

# calculate predicted price
predicted_price = model.predict(X_test)

# display predicted price
print("Predicted price:", predicted_price)